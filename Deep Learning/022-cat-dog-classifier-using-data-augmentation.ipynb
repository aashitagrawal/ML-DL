{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc9997b6",
   "metadata": {},
   "source": [
    "# Today's Date - 30 June 2023\n",
    "# Topic - Training with and without Data Augmentation on large batchs of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70366208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b28d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "971201a9",
   "metadata": {},
   "source": [
    "## 1000 images of dogs and cats each for data augmentation\n",
    "## 500 images of dogs and cats each for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4c9ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_directory = 'A:/CODING/Python/Projects/cat-dog-classifier/data/train'\n",
    "valid_directory = 'A:/CODING/Python/Projects/cat-dog-classifier/data/valid'\n",
    "\n",
    "train_augmented_images = train_datagen.flow_from_directory(\n",
    "    directory=train_directory,\n",
    "    target_size=(150,150),\n",
    "    batch_size = batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "validation_images = test_datagen.flow_from_directory(\n",
    "    directory=valid_directory,\n",
    "    target_size=(150,150),\n",
    "    batch_size = batch_size,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a39a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(150, 150, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d353ecc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 74, 74, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 72, 72, 32)        9248      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 72, 72, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 36, 36, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 34, 34, 64)        18496     \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 34, 34, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 17, 17, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 18496)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                1183808   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,212,513\n",
      "Trainable params: 1,212,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ce1dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3d0a59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aashi\\AppData\\Local\\Temp\\ipykernel_13464\\4047799208.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "125/125 [==============================] - 52s 408ms/step - loss: 0.7066 - accuracy: 0.5090 - val_loss: 0.6830 - val_accuracy: 0.5838\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - 27s 216ms/step - loss: 0.6747 - accuracy: 0.5985 - val_loss: 0.7530 - val_accuracy: 0.5163\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - 26s 204ms/step - loss: 0.6384 - accuracy: 0.6465 - val_loss: 0.6087 - val_accuracy: 0.6913\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.6081 - accuracy: 0.6745 - val_loss: 0.6004 - val_accuracy: 0.6825\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.5832 - accuracy: 0.6985 - val_loss: 0.5812 - val_accuracy: 0.6975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fbf3233970>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_augmented_images,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=5,\n",
    "        validation_data=validation_images,\n",
    "        validation_steps=800 // batch_size,\n",
    "        verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a5515cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cb0848f",
   "metadata": {},
   "source": [
    "# We achevied 69% percent accuracy in just 5 epochs\n",
    "### If trained for more epochs it will reach higher"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8474f825",
   "metadata": {},
   "source": [
    "\n",
    "# Lets try to train without data augmentation, and compare the results\n",
    "### 1000 images of cats and dogs each for train\n",
    "### 500 images of cats and dogs each for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36691a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n",
      "Found 1000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_directory = 'A:/CODING/Python/Projects/cat-dog-classifier/data/train'\n",
    "valid_directory = 'A:/CODING/Python/Projects/cat-dog-classifier/data/valid'\n",
    "\n",
    "train = keras.utils.image_dataset_from_directory(\n",
    "    directory=train_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=16,\n",
    "    image_size=(150, 150)  # all images will be reshaped into this size\n",
    ")\n",
    "\n",
    "validation = keras.utils.image_dataset_from_directory(\n",
    "    directory=valid_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=16,\n",
    "    image_size=(150, 150)  # all images will be reshaped into this size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fe5c020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c411842f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First image in the batch:\n",
      "tf.Tensor(\n",
      "[[[180.45233  173.45233  129.45233 ]\n",
      "  [183.02733  176.02733  132.02733 ]\n",
      "  [184.46167  177.46167  133.46167 ]\n",
      "  ...\n",
      "  [182.64833  185.64833  140.64833 ]\n",
      "  [187.87036  190.87036  145.87036 ]\n",
      "  [188.       191.       146.      ]]\n",
      "\n",
      " [[185.107    178.107    134.107   ]\n",
      "  [184.98799  177.98799  133.98799 ]\n",
      "  [185.56     178.56     134.56    ]\n",
      "  ...\n",
      "  [183.       186.       141.      ]\n",
      "  [186.99902  189.99902  144.99902 ]\n",
      "  [188.       191.       146.      ]]\n",
      "\n",
      " [[185.64     178.64     134.64    ]\n",
      "  [182.92168  175.92168  131.92168 ]\n",
      "  [182.34166  175.34166  131.34166 ]\n",
      "  ...\n",
      "  [183.74167  186.74167  141.74167 ]\n",
      "  [185.90002  188.90002  143.90002 ]\n",
      "  [187.66168  190.66168  145.66168 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[160.56502  155.56502  123.56501 ]\n",
      "  [156.33165  151.33165  119.33166 ]\n",
      "  [156.70831  151.70831  119.708305]\n",
      "  ...\n",
      "  [149.       152.       123.      ]\n",
      "  [150.       153.       124.      ]\n",
      "  [151.       154.       125.      ]]\n",
      "\n",
      " [[160.63599  155.63599  123.636   ]\n",
      "  [155.021    150.021    118.021   ]\n",
      "  [153.945    148.945    116.945   ]\n",
      "  ...\n",
      "  [149.       152.       123.      ]\n",
      "  [150.       153.       124.      ]\n",
      "  [151.       154.       125.      ]]\n",
      "\n",
      " [[156.38998  151.38998  119.389984]\n",
      "  [157.40668  152.40668  120.40668 ]\n",
      "  [153.5      148.5      116.5     ]\n",
      "  ...\n",
      "  [149.       152.       123.      ]\n",
      "  [150.       153.       124.      ]\n",
      "  [151.       154.       125.      ]]], shape=(150, 150, 3), dtype=float32)\n",
      "First label in the batch:\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train:\n",
    "    print('First image in the batch:')\n",
    "    print(images[0])  # Print the first image in the batch\n",
    "\n",
    "    print('First label in the batch:')\n",
    "    print(labels[0])  # Print the first label in the batch\n",
    "\n",
    "    break  # Print only the first batch for demonstration purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8582cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the values in numpy array has range of 0 to 255 (each pixel has 0 to 255 brightness range)\n",
    "\n",
    "# So, we need to normalize\n",
    "def process (image,label):\n",
    "  image = tf.cast(image/255, tf.float32)\n",
    "  return image,label\n",
    "\n",
    "train = train.map(process)\n",
    "validation = validation.map(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9513d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First image in the batch:\n",
      "tf.Tensor(\n",
      "[[[0.79428035 0.90408427 0.9511431 ]\n",
      "  [0.8012638  0.9168716  0.9610285 ]\n",
      "  [0.8111111  0.92784315 0.9737255 ]\n",
      "  ...\n",
      "  [0.4762342  0.5302211  0.38136432]\n",
      "  [0.3097035  0.35966432 0.24813476]\n",
      "  [0.4248857  0.4719445  0.38373542]]\n",
      "\n",
      " [[0.7709587  0.88076264 0.9238999 ]\n",
      "  [0.79377806 0.90700626 0.9501435 ]\n",
      "  [0.8058993  0.92033076 0.9638431 ]\n",
      "  ...\n",
      "  [0.23033671 0.2821759  0.15080716]\n",
      "  [0.35338458 0.40215555 0.3074888 ]\n",
      "  [0.46773985 0.51358736 0.4376484 ]]\n",
      "\n",
      " [[0.8089904  0.91487277 0.9501669 ]\n",
      "  [0.817651   0.9235333  0.95882744]\n",
      "  [0.7976972  0.9044946  0.94161874]\n",
      "  ...\n",
      "  [0.38177824 0.4279068  0.32847533]\n",
      "  [0.4301672  0.47614107 0.40206647]\n",
      "  [0.48884404 0.53191596 0.4753077 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.61637354 0.58924985 0.620426  ]\n",
      "  [0.5787589  0.5573423  0.58566487]\n",
      "  [0.39359498 0.38376072 0.41019627]\n",
      "  ...\n",
      "  [0.575621   0.5950979  0.6188237 ]\n",
      "  [0.565426   0.5849029  0.6086287 ]\n",
      "  [0.573845   0.5933219  0.6111392 ]]\n",
      "\n",
      " [[0.60186577 0.56555206 0.59987885]\n",
      "  [0.49737504 0.46992406 0.5012966 ]\n",
      "  [0.36247203 0.35070732 0.3781583 ]\n",
      "  ...\n",
      "  [0.5644     0.591851   0.6153804 ]\n",
      "  [0.5453991  0.5738697  0.5943404 ]\n",
      "  [0.5623836  0.59375614 0.60552084]]\n",
      "\n",
      " [[0.5081514  0.4718377  0.5061645 ]\n",
      "  [0.4127055  0.38525453 0.41662708]\n",
      "  [0.36088878 0.34912407 0.37657505]\n",
      "  ...\n",
      "  [0.54948723 0.5769382  0.60831076]\n",
      "  [0.54615265 0.57360363 0.59867543]\n",
      "  [0.5948297  0.62228066 0.64735246]]], shape=(150, 150, 3), dtype=float32)\n",
      "First label in the batch:\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train:\n",
    "    print('First image in the batch:')\n",
    "    print(images[0])  # Print the first image in the batch\n",
    "\n",
    "    print('First label in the batch:')\n",
    "    print(labels[0])  # Print the first label in the batch\n",
    "\n",
    "    break  # Print only the first batch for demonstration purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bb7d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(150, 150, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88ef8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa49d346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "125/125 [==============================] - 29s 224ms/step - loss: 0.7128 - accuracy: 0.5340 - val_loss: 0.7286 - val_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - 24s 188ms/step - loss: 0.6599 - accuracy: 0.5940 - val_loss: 0.6159 - val_accuracy: 0.6790\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - 24s 188ms/step - loss: 0.6102 - accuracy: 0.6675 - val_loss: 0.5964 - val_accuracy: 0.6750\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.5751 - accuracy: 0.6900 - val_loss: 0.6001 - val_accuracy: 0.6760\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.5383 - accuracy: 0.7345 - val_loss: 0.5771 - val_accuracy: 0.7100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fbf32ea620>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, epochs=5, validation_data=validation, verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b14d706",
   "metadata": {},
   "source": [
    "# On training on higher number of epochs, augmented data will give higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3384bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
